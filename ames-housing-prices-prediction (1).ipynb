{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # pattern searching\nimport seaborn as sns # statistical plots\nimport sklearn\nimport matplotlib.pyplot as plt # plots\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-29T12:47:32.792166Z","iopub.execute_input":"2022-06-29T12:47:32.792422Z","iopub.status.idle":"2022-06-29T12:47:33.828948Z","shell.execute_reply.started":"2022-06-29T12:47:32.792393Z","shell.execute_reply":"2022-06-29T12:47:33.827481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col='Id')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:33.830562Z","iopub.execute_input":"2022-06-29T12:47:33.83136Z","iopub.status.idle":"2022-06-29T12:47:33.901033Z","shell.execute_reply.started":"2022-06-29T12:47:33.831322Z","shell.execute_reply":"2022-06-29T12:47:33.900298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropping the outliers","metadata":{}},{"cell_type":"markdown","source":"After plotting few critical parameters such as gross living area against house sales price we can see that there are some outliers which can be really difficult to accommodate in any prediction models. The best way to deal with them is to remove them from the dataset for the sake of modeling","metadata":{}},{"cell_type":"code","source":"is_outlier = (df_train['GrLivArea']>4000)&(df_train['SalePrice']<700000)\n# sns.scatterplot(data=df_train['total_bill'], hue=is_outlier)\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.scatterplot(data=df_train, x='GrLivArea', y='SalePrice',hue=is_outlier, ax=ax, legend = False);\n# ax.axhline(y=300000 , color='Black', dashes=(4,1,2,1))\n# ax.axvline(x=4000, color='Grey', dashes=(2,1,1,1))\n# ax.legend(False)\n# ax.fill_between(4000, 30000,facecolor ='black', alpha = 0.8)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:33.904417Z","iopub.execute_input":"2022-06-29T12:47:33.904618Z","iopub.status.idle":"2022-06-29T12:47:34.212324Z","shell.execute_reply.started":"2022-06-29T12:47:33.904593Z","shell.execute_reply":"2022-06-29T12:47:34.21161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(\n    df_train[(df_train['GrLivArea']>4000) \n        &    (df_train['SalePrice']<700000)].index)\nfig, ax = plt.subplots(figsize=(12,8))\nsns.scatterplot(data=df_train, x='GrLivArea', y='SalePrice', ax=ax);\n# ax.axhline(y=300000 , color='Black', dashes=(4,1,2,1))\n# ax.axvline(x=4000, color='Grey', dashes=(2,1,1,1))","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:34.214127Z","iopub.execute_input":"2022-06-29T12:47:34.214366Z","iopub.status.idle":"2022-06-29T12:47:34.442369Z","shell.execute_reply.started":"2022-06-29T12:47:34.214336Z","shell.execute_reply":"2022-06-29T12:47:34.441699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After removing the outliers we created a copy of train data (except sales price) and test data and appended them to make a total feature dataset","metadata":{}},{"cell_type":"code","source":"train_copy = df_train.copy().drop('SalePrice', axis=1)\ntest_copy = df_test.copy()\ndf_total = pd.concat([train_copy, test_copy])\ndf_total.to_csv('df_total.csv')\ndf_total.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:34.443388Z","iopub.execute_input":"2022-06-29T12:47:34.443748Z","iopub.status.idle":"2022-06-29T12:47:34.571166Z","shell.execute_reply.started":"2022-06-29T12:47:34.443716Z","shell.execute_reply":"2022-06-29T12:47:34.57039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some features have incorrect datatypes and lot of features have missing values. Feature set needs dtype conversion and imputation before it can be used for modelling","metadata":{}},{"cell_type":"markdown","source":"## Converting dtypes","metadata":{}},{"cell_type":"code","source":"##converting the numerical columns into object columns\ndf_total['GarageYrBlt'] = df_total['GarageYrBlt'].fillna(0).astype('int64')\ndf_total['MSSubClass'] = df_total['MSSubClass'].astype('object')\ndf_total['MoSold'] = df_total['MoSold'].astype('object', copy=False)\ndf_total['YrSold'] = df_total['YrSold'].astype('object', copy=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:34.572254Z","iopub.execute_input":"2022-06-29T12:47:34.573149Z","iopub.status.idle":"2022-06-29T12:47:34.581922Z","shell.execute_reply.started":"2022-06-29T12:47:34.573112Z","shell.execute_reply":"2022-06-29T12:47:34.5812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Primary Imputation","metadata":{}},{"cell_type":"markdown","source":"In the first stage we will use logical assumptions to impute the missing data.\n\n1. MS Zoning: We can fairly assume that Houses in the same residential zone will possibly have same housing subclass and neighborhood. Based MSSubClass and Neighborhood of the record (with missing MSZoning value), we can impute the missing value with mode value for that combination of MSSubClass and Neighborhood e.g. If MSSubClass and Neighborhood for a record with missing MSZoning value are 150 and North Ames we find the mode MSZoning value for all the records with 150 MSSubClass and North Ames Neighborhood which lets assume is Residential Low Density. Then we impute this Residential Low Density as MSZoning value for all the records where MSZoning value was missing and MSSubClass and Neighborhood were 150 and North Ames\n2. LotFrontage: We will use similar strategy as mentioned above for MSZoning. For missing LotFrontage, we will impute median LotFrontage corresponding the same MSZoning\n3. Masonry Veneer Area and Type: There is no logical assumption which can be leveraged for this feature.We will simply input the missing area value with 0 and missing type value with None\n4. Basement features: There are lot of basement features missing where the basement area is zero and we can impute the missing features as 'NA' (categorical) or '0'(numerical)\n5. Garage features: There are lot of basement features missing where the garage area is zero and we can impute the missing features as 'NA' (categorical) or '0'(numerical)\n6. Utilities: All but one value is missing. And majority of the values are 'AllPub' (2916 out of 2919). We use 'AllPub' to impute missing values\n7. Exterior features (1st and 2nd): We use the same strategy as we used for MSZoning. For missing exterior features we impute mode Exterior features corresponding to same MSSubClass and Neighborhood\n8. Electrical, Functional, Kitchen features: We use the absolute mode values to impute the missing values\n9. Fireplaces, Pool and Miscellenious features: Where fireplace/pool area/misc feature value is zero, the corresponding categorical features will be imputed as 'NA'. There are records where pool area is not zero but pool quality is missing. In that case we use mode pool quality value. There are records where miscellenious feature value is non zero but the miscellenious feature value is missing. In that case we use the 'Othr' value for imputation \n10. SaleType: We use absolute mode values to impute the missing values\n11. Alley and Fence: Most of the records have missing values for these two feature. We will discard these column from further preprocessing","metadata":{}},{"cell_type":"code","source":"##MSZoning: Mode MSZoning values sharing same MSSubclass and Neighborhood \ndf_total['MSZoning']=df_total.groupby(['MSSubClass', 'Neighborhood'])['MSZoning'].transform(lambda x:x.fillna(x.mode()[0]))\n\n##Lot Features\ndf_total['LotFrontage']=df_total.groupby(\"MSZoning\")['LotFrontage'].transform(lambda x:x.fillna(x.median()))\n\n##Masonry Veneer Features\ndf_total['MasVnrArea'].where(df_total['MasVnrArea'].notna(), 0, inplace=True)\ndf_total['MasVnrType'].where(df_total['MasVnrType'].notna(), \"None\", inplace=True)\n\n\n##Basement Feautres\ndf_total['BsmtQual'].where(df_total['TotalBsmtSF']!=0, \"NA\", inplace=True)\ndf_total['BsmtCond'].where(df_total['TotalBsmtSF']!=0, \"NA\", inplace=True)\ndf_total['BsmtExposure'].where(df_total['TotalBsmtSF']!=0, \"NA\", inplace=True)\ndf_total['BsmtFinType1'].where(df_total['TotalBsmtSF']!=0, \"NA\", inplace=True)\ndf_total['BsmtFinType2'].where(df_total['TotalBsmtSF']!=0, \"NA\", inplace=True)\ndf_total['BsmtFullBath'].where(df_total['TotalBsmtSF']!=0, 0.0, inplace=True)\ndf_total['BsmtHalfBath'].where(df_total['TotalBsmtSF']!=0, 0.0, inplace=True)\ndf_total['BsmtFinSF1'].where(df_total['TotalBsmtSF']!=0, 0.0, inplace=True)\ndf_total['BsmtFinSF2'].where(df_total['TotalBsmtSF']!=0, 0.0, inplace=True)\ndf_total['BsmtUnfSF'].where(df_total['TotalBsmtSF']!=0, 0.0, inplace=True)\n\n\n##Garage Features\ndf_total['GarageCond'].where(df_total['GarageArea']!=0, \"NA\", inplace=True)\ndf_total['GarageQual'].where(df_total['GarageArea']!=0, \"NA\", inplace=True)\ndf_total['GarageFinish'].where(df_total['GarageArea']!=0, \"NA\", inplace=True)\ndf_total['GarageType'].where(df_total['GarageArea']!=0, \"NA\", inplace=True)\n\n\n##Utilities: 2916 out of 2917 realized utilities values are 'AllPub'. We will just use the mode to fill NA values\ndf_total['Utilities']=df_total['Utilities'].fillna(df_total['Utilities'].mode()[0])\n\n\n##Exterior1st and Exterior2nd: Mode Exterior1st and Exterior2nd values sharing same MSSubclass and Neighborhood \ndf_total['Exterior1st']=df_total.groupby(['MSSubClass', 'Neighborhood'])['Exterior1st'].transform(lambda x:x.fillna(x.mode()[0]))\ndf_total['Exterior2nd']=df_total.groupby(['MSSubClass', 'Neighborhood'])['Exterior2nd'].transform(lambda x:x.fillna(x.mode()[0]))\n\n\n##Electrical\ndf_total['Electrical']=df_total['Electrical'].fillna(df_total['Electrical'].mode()[0])\n\n\n##KitchenQual\ndf_total['KitchenQual']=df_total['KitchenQual'].fillna(df_total['KitchenQual'].mode()[0])\n\n\n##Functional\ndf_total['Functional']=df_total['Functional'].fillna(df_total['Functional'].mode()[0])\n\n\n##Fireplace Features\ndf_total['FireplaceQu'].where(df_total['Fireplaces']!=0, \"NA\", inplace=True)\n\n\n##Pool Features\ndf_total['PoolQC'].where(df_total['PoolArea']!=0, \"NA\", inplace=True)\ndf_total['PoolQC']=df_total['PoolQC'].fillna(df_total['PoolQC'][df_total['PoolArea']!=0].mode()[0])\n\n\n##Miscellaneous Features\ndf_total['MiscFeature'].where(df_total['MiscVal']!=0, \"NA\", inplace=True)\ndf_total['MiscFeature']=df_total['MiscFeature'].fillna('Othr')\n\n\n##SaleType\ndf_total['SaleType']=df_total['SaleType'].fillna(df_total['SaleType'].mode()[0])\n\n\n##Too many missing values in Alley and Fence feature and no logical step to impute. We will drop them. \n##Using errors='ignore' will suppress errors if non-existing rows are dropped\ndf_total.drop(axis=1, columns=['Alley', 'Fence'], errors='ignore', inplace=True)\n\n\n##Missing values after primary imputation\ndf_total.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:34.583369Z","iopub.execute_input":"2022-06-29T12:47:34.583745Z","iopub.status.idle":"2022-06-29T12:47:34.870497Z","shell.execute_reply.started":"2022-06-29T12:47:34.583595Z","shell.execute_reply":"2022-06-29T12:47:34.869667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Secondary Imputation","metadata":{}},{"cell_type":"markdown","source":"### Basement Features","metadata":{}},{"cell_type":"markdown","source":"There are records where either all of the basement features are missing or the basement area is non zero but corresponding categorical features are missing in that case. In former case we just assume that house does not have any basement and impute numerical values as '0' and categorical values as 'NA' and in latter case we just impute absolute mode value for all the missing features","metadata":{}},{"cell_type":"code","source":"##Id 2121 has no data for all the basement features. We will assume that it does not has any basement\ndf_total.iloc[2120,[28,29,30,31,33]] = 'NA'\ndf_total.iloc[2120,[32,34,35,36,45,46]] = 0.0\n\ndf_total.loc[2121, ['BsmtFinType1',\n 'BsmtFinSF1',\n 'BsmtFinSF2',\n 'BsmtUnfSF',\n 'TotalBsmtSF',\n 'BsmtFullBath',\n 'BsmtHalfBath']]=0.0\n\n##BsmtQual: Mode BsmtQual values for NaN\ndf_total['BsmtQual']=df_total['BsmtQual'].transform(lambda x: x.fillna(x.mode()[0]))\n\n##BsmtCond: Mode BsmtCond values for NaN\ndf_total['BsmtCond']=df_total['BsmtCond'].transform(lambda x: x.fillna(x.mode()[0]))\n\n##BsmtExposure: Mode BsmtExposure values for NaN\ndf_total['BsmtExposure']=df_total['BsmtExposure'].transform(lambda x: x.fillna(x.mode()[0]))\n\n##BsmtFinTyoe2: Mode BsmtFinType2 values (that have non zero BsmtFinSF2 which is less than 500) for NaN values\ndf_total['BsmtFinType2']=df_total['BsmtFinType2'].transform(lambda x: x.fillna('Rec'))\n\ndf_total.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:34.872069Z","iopub.execute_input":"2022-06-29T12:47:34.872342Z","iopub.status.idle":"2022-06-29T12:47:34.914855Z","shell.execute_reply.started":"2022-06-29T12:47:34.872308Z","shell.execute_reply":"2022-06-29T12:47:34.914042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Garage Features","metadata":{"execution":{"iopub.status.busy":"2022-03-09T05:30:38.171052Z","iopub.execute_input":"2022-03-09T05:30:38.171354Z","iopub.status.idle":"2022-03-09T05:30:38.195612Z","shell.execute_reply.started":"2022-03-09T05:30:38.171322Z","shell.execute_reply":"2022-03-09T05:30:38.194329Z"}}},{"cell_type":"markdown","source":"There are two records where garage features are missing. In one case the garage area is non zero and in other case garage area is not available.\nBoth of these garages are detached type. Using this as a common feature we find mode values to impute missing garage quality, finish, condition and cars. \nAfter we find the garage cars value for the house where area is missing we can impute the mean garage area where type is detached and garage cars value is same as in case of missing garage area record. ","metadata":{}},{"cell_type":"code","source":"##GarageFinish\ndf_total['GarageFinish']=df_total['GarageFinish'].fillna(df_total['GarageFinish'][df_total['GarageType']=='Detchd'].mode()[0])\n\n##GarageQual\ndf_total['GarageQual']=df_total['GarageQual'].fillna(df_total['GarageQual'][df_total['GarageType']=='Detchd'].mode()[0])\n\n##GarageCond\ndf_total['GarageCond']=df_total['GarageCond'].fillna(df_total['GarageCond'][df_total['GarageType']=='Detchd'].mode()[0])\n\n##GarageCars\ndf_total['GarageCars']=df_total['GarageCars'].fillna(df_total['GarageCars'][df_total['GarageType']=='Detchd'].mode()[0])\n\n##GarageArea\ndf_total['GarageArea']=df_total['GarageArea'].fillna(df_total['GarageArea'][(df_total['GarageType']=='Detchd')&(df_total['GarageCars']==2.0)].mean())\n\ndf_total.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:34.916346Z","iopub.execute_input":"2022-06-29T12:47:34.916595Z","iopub.status.idle":"2022-06-29T12:47:34.957477Z","shell.execute_reply.started":"2022-06-29T12:47:34.916562Z","shell.execute_reply":"2022-06-29T12:47:34.956263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding the ordinal features into numerical features","metadata":{}},{"cell_type":"markdown","source":"To process the ordinal features in the models we need to convert them into numerical features first. We will assume one step integer increment for each ordinal category.","metadata":{}},{"cell_type":"code","source":"df_total = df_total.replace({\"BsmtCond\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"NA\" : 0,\"No\":1, \"Mn\" : 2, \"Av\": 3, \"Gd\" : 4},\n                       \"BsmtFinType1\" : {\"NA\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"NA\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageFinish\" : {\"NA\" : 0, \"Unf\" : 1, \"RFn\" : 2, \"Fin\" : 3},\n                             \"GarageCond\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"PavedDrive\" : {\"NA\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"NA\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                        \"CentralAir\":{\"N\":0,\"Y\":1}\n                            })","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:34.961098Z","iopub.execute_input":"2022-06-29T12:47:34.96129Z","iopub.status.idle":"2022-06-29T12:47:35.033335Z","shell.execute_reply.started":"2022-06-29T12:47:34.961266Z","shell.execute_reply":"2022-06-29T12:47:35.032643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For correlation and skewness measurement let us create seperate dataframe for numerical and categorical features","metadata":{}},{"cell_type":"code","source":"df_num_ord = df_total.select_dtypes(include=['int64','float64'])\ndf_cat = df_total.select_dtypes(include=['object']).astype(str)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:35.034712Z","iopub.execute_input":"2022-06-29T12:47:35.034964Z","iopub.status.idle":"2022-06-29T12:47:35.048402Z","shell.execute_reply.started":"2022-06-29T12:47:35.034932Z","shell.execute_reply":"2022-06-29T12:47:35.047582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation between numerical and ordinal columns","metadata":{}},{"cell_type":"markdown","source":"The input features to be used in the regression model should be atleast fairly uncorrelated if not completed independent. We will visualize a correlation matrix. Since some of the numerical features are ordinal, we will use spearman correlation coefficient. Spearman coefficient captures the correlation even when it is fairly qualitative.","metadata":{}},{"cell_type":"code","source":"corr = df_num_ord.corr(method='spearman')\nkot = corr[((corr>=.7) | (corr<=-0.7)) & (corr!=1.0)]\nfig, ax = plt.subplots(figsize=(18,18))\nsns.heatmap(kot,ax=ax,center=0,annot=True,fmt='0.2f',cbar=False,cmap=sns.diverging_palette(110,110,s=100,l=60,center='light',as_cmap=True));\nfig.savefig('num_corr.jpeg', dpi=1200, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:35.049437Z","iopub.execute_input":"2022-06-29T12:47:35.049625Z","iopub.status.idle":"2022-06-29T12:47:53.140554Z","shell.execute_reply.started":"2022-06-29T12:47:35.049602Z","shell.execute_reply":"2022-06-29T12:47:53.139868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will define a function which will accept the dataset and threshold correlation coefficient as input and will return the list of columns having correlation coefficient with atleast one other column more than the threshold","metadata":{}},{"cell_type":"code","source":"def correlation(dataset, threshold):\n    col_corr = set()\n#     row_corr = set()\n    # Set of all the names of correlated columns\n    corr_matrix = dataset.corr(method='spearman')\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n#                 rowname = corr_matrix.columns[j]\n                col_corr.add(colname)\n#                 row_corr.add(rowname)\n    return col_corr","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:53.143511Z","iopub.execute_input":"2022-06-29T12:47:53.143867Z","iopub.status.idle":"2022-06-29T12:47:53.150858Z","shell.execute_reply.started":"2022-06-29T12:47:53.143837Z","shell.execute_reply":"2022-06-29T12:47:53.149506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_features = correlation(df_num_ord, 0.7)\ncorr_features","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:53.152059Z","iopub.execute_input":"2022-06-29T12:47:53.152419Z","iopub.status.idle":"2022-06-29T12:47:53.217741Z","shell.execute_reply.started":"2022-06-29T12:47:53.152381Z","shell.execute_reply":"2022-06-29T12:47:53.217047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num_ord.drop(corr_features, axis=1, inplace=True, errors='ignore')\ndf_total.drop(corr_features, axis=1, inplace=True, errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:53.218996Z","iopub.execute_input":"2022-06-29T12:47:53.219398Z","iopub.status.idle":"2022-06-29T12:47:53.23206Z","shell.execute_reply.started":"2022-06-29T12:47:53.21936Z","shell.execute_reply":"2022-06-29T12:47:53.231144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Skewness of sales price and featureset","metadata":{}},{"cell_type":"code","source":"sns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:53.233178Z","iopub.execute_input":"2022-06-29T12:47:53.233847Z","iopub.status.idle":"2022-06-29T12:47:54.019285Z","shell.execute_reply.started":"2022-06-29T12:47:53.233812Z","shell.execute_reply":"2022-06-29T12:47:54.018617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Sales Price distribution is skewed with heavy right tail. Since one of the common assumption of Linear Regression is Normal Distribution of errors (which translates to normal distribution of output variable given input variables and normal distribution of the errors), we will try to transform output variable to Normal Distribution [1]. We will try log1p transform and check the Q-Q plot for normality","metadata":{}},{"cell_type":"code","source":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:54.020511Z","iopub.execute_input":"2022-06-29T12:47:54.020774Z","iopub.status.idle":"2022-06-29T12:47:54.502181Z","shell.execute_reply.started":"2022-06-29T12:47:54.020729Z","shell.execute_reply":"2022-06-29T12:47:54.501501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Applying log transform made the sales price near normally distributed.","metadata":{}},{"cell_type":"markdown","source":"### One hot encoding","metadata":{}},{"cell_type":"markdown","source":"Finally we need to perform one hot encoding of the categorical variable. To avoid multicollinearity among the input variables we will drop the first column of every one hot encoded variable. ","metadata":{}},{"cell_type":"code","source":"df_total = pd.get_dummies(df_total,drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:54.503361Z","iopub.execute_input":"2022-06-29T12:47:54.503602Z","iopub.status.idle":"2022-06-29T12:47:54.542878Z","shell.execute_reply.started":"2022-06-29T12:47:54.503568Z","shell.execute_reply":"2022-06-29T12:47:54.542157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:54.544282Z","iopub.execute_input":"2022-06-29T12:47:54.544513Z","iopub.status.idle":"2022-06-29T12:47:54.751595Z","shell.execute_reply.started":"2022-06-29T12:47:54.544481Z","shell.execute_reply":"2022-06-29T12:47:54.75088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing training and testing data","metadata":{}},{"cell_type":"code","source":"#records and features in the training dataset\nn_train = df_train.index[-1] \n\nX=df_total.loc[:n_train]\ny=df_train['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:54.753095Z","iopub.execute_input":"2022-06-29T12:47:54.753398Z","iopub.status.idle":"2022-06-29T12:47:54.764537Z","shell.execute_reply.started":"2022-06-29T12:47:54.753364Z","shell.execute_reply":"2022-06-29T12:47:54.76376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plain Linear Regression","metadata":{}},{"cell_type":"code","source":"LR = LinearRegression()\nLR.fit(X_train, y_train)\ny_pred=LR.predict(X_test)\n\nr2 = r2_score(y_test, y_pred)\nrmse = np.sqrt(np.mean((y_test-y_pred)**2))\n#ysub_pred = np.expm1(LR.predict(df_total.loc[n_train+1:]))\n#submission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\n#submission.to_csv('LR_final.csv')\nr2, rmse","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:54.765707Z","iopub.execute_input":"2022-06-29T12:47:54.766275Z","iopub.status.idle":"2022-06-29T12:47:54.827054Z","shell.execute_reply.started":"2022-06-29T12:47:54.766239Z","shell.execute_reply":"2022-06-29T12:47:54.826356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residual = y_test - y_pred\nplt.scatter(y_pred, residual)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:54.828191Z","iopub.execute_input":"2022-06-29T12:47:54.829494Z","iopub.status.idle":"2022-06-29T12:47:55.049751Z","shell.execute_reply.started":"2022-06-29T12:47:54.829457Z","shell.execute_reply":"2022-06-29T12:47:55.049056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Residual vs output variable plot determines the extent of homoscedasticity of the residuals. The above plot shows that residuals are not perfectly homoscedastic although the range of residual variance is quite narrow. We will explore other regression models as well and check its homoscedasticity. ","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Regression","metadata":{}},{"cell_type":"code","source":"RF = RandomForestRegressor(n_estimators=100, max_depth=12)\nRF.fit(X_train, y_train)\ny_pred=RF.predict(X_test)\n\nr2 = r2_score(y_test, y_pred)\nrmse = np.sqrt(np.mean((y_test-y_pred)**2))\nysub_pred = np.expm1(RF.predict(df_total.loc[n_train+1:]))\nsubmission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\nsubmission.to_csv('RF_final.csv')\nr2, rmse\nr2, rmse","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:55.050939Z","iopub.execute_input":"2022-06-29T12:47:55.051171Z","iopub.status.idle":"2022-06-29T12:47:56.443184Z","shell.execute_reply.started":"2022-06-29T12:47:55.051138Z","shell.execute_reply":"2022-06-29T12:47:56.442497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residual = y_test - y_pred\nplt.scatter(y_pred, residual)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:56.444447Z","iopub.execute_input":"2022-06-29T12:47:56.444697Z","iopub.status.idle":"2022-06-29T12:47:56.634079Z","shell.execute_reply.started":"2022-06-29T12:47:56.444664Z","shell.execute_reply":"2022-06-29T12:47:56.633416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ridge Regression","metadata":{"execution":{"iopub.status.busy":"2022-03-19T21:35:26.18418Z","iopub.execute_input":"2022-03-19T21:35:26.184451Z","iopub.status.idle":"2022-03-19T21:35:26.198266Z","shell.execute_reply.started":"2022-03-19T21:35:26.184422Z","shell.execute_reply":"2022-03-19T21:35:26.197203Z"}}},{"cell_type":"code","source":"Rdge = Ridge(alpha=10)\nRdge.fit(X_train, y_train)\ny_pred=Rdge.predict(X_test)\n\nysub_pred = np.expm1(Rdge.predict(df_total.loc[n_train+1:]))\nsubmission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\nsubmission.to_csv('Rdge_final.csv')\nr2 = r2_score(y_test, y_pred)\nrmse = np.sqrt(np.mean((y_test - y_pred)**2))\nr2, rmse","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:56.635277Z","iopub.execute_input":"2022-06-29T12:47:56.635505Z","iopub.status.idle":"2022-06-29T12:47:56.72314Z","shell.execute_reply.started":"2022-06-29T12:47:56.635473Z","shell.execute_reply":"2022-06-29T12:47:56.722445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residual = y_test - y_pred\nplt.scatter(y_pred, residual)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:56.727437Z","iopub.execute_input":"2022-06-29T12:47:56.730594Z","iopub.status.idle":"2022-06-29T12:47:56.924121Z","shell.execute_reply.started":"2022-06-29T12:47:56.730551Z","shell.execute_reply":"2022-06-29T12:47:56.923481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lasso Regression","metadata":{}},{"cell_type":"code","source":"Lsso = Lasso(alpha=0.5)\nLsso.fit(X_train, y_train)\ny_pred=Lsso.predict(X_test)\n\nysub_pred = np.expm1(Lsso.predict(df_total.loc[n_train+1:]))\nsubmission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\nsubmission.to_csv('Lsso_final.csv')\nr2 = r2_score(y_test, y_pred)\nrmse = np.sqrt(np.mean((y_test - y_pred)**2))\nr2, rmse","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:56.925478Z","iopub.execute_input":"2022-06-29T12:47:56.925729Z","iopub.status.idle":"2022-06-29T12:47:56.974717Z","shell.execute_reply.started":"2022-06-29T12:47:56.925695Z","shell.execute_reply":"2022-06-29T12:47:56.973966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residual = y_test - y_pred\nplt.scatter(y_pred, residual)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:56.978999Z","iopub.execute_input":"2022-06-29T12:47:56.980099Z","iopub.status.idle":"2022-06-29T12:47:57.198456Z","shell.execute_reply.started":"2022-06-29T12:47:56.98006Z","shell.execute_reply":"2022-06-29T12:47:57.197796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Elasic Net Regression","metadata":{}},{"cell_type":"code","source":"EN = ElasticNet(alpha=5, l1_ratio=0.05)\nEN.fit(X_train, y_train)\ny_pred=EN.predict(X_test)\n\nysub_pred = np.expm1(EN.predict(df_total.loc[n_train+1:]))\nsubmission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\nsubmission.to_csv('Lsso_final.csv')\nr2 = r2_score(y_test, y_pred)\nrmse = np.sqrt(np.mean((y_test - y_pred)**2))\nr2, rmse","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:57.19953Z","iopub.execute_input":"2022-06-29T12:47:57.200379Z","iopub.status.idle":"2022-06-29T12:47:57.248135Z","shell.execute_reply.started":"2022-06-29T12:47:57.20034Z","shell.execute_reply":"2022-06-29T12:47:57.247445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residual = y_test - y_pred\nplt.scatter(y_pred, residual)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:57.25245Z","iopub.execute_input":"2022-06-29T12:47:57.254868Z","iopub.status.idle":"2022-06-29T12:47:57.453102Z","shell.execute_reply.started":"2022-06-29T12:47:57.254826Z","shell.execute_reply":"2022-06-29T12:47:57.45242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:20px;\"> \n    Let us use cross validation, and grid search to find best parameters for ridge, lasso, elasticnet\n</span>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:57.454158Z","iopub.execute_input":"2022-06-29T12:47:57.455003Z","iopub.status.idle":"2022-06-29T12:47:57.460093Z","shell.execute_reply.started":"2022-06-29T12:47:57.454964Z","shell.execute_reply":"2022-06-29T12:47:57.458216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = {'alpha' : [13]}\nscoring = ['neg_mean_squared_error', 'r2']\nridgeCV = GridSearchCV(Ridge(), param_grid=parameters, scoring=scoring, refit='r2', cv=5).fit(X, y)\nprint(f\"Best Ridge Regressor is {ridgeCV.best_estimator_} and corresponding r2 score on prediction is {ridgeCV.best_score_}\")\n\nysub_pred = np.expm1(ridgeCV.predict(df_total.loc[n_train+1:]))\nsubmission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\nsubmission.to_csv('ridgeCV_final.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:57.461268Z","iopub.execute_input":"2022-06-29T12:47:57.461549Z","iopub.status.idle":"2022-06-29T12:47:57.702794Z","shell.execute_reply.started":"2022-06-29T12:47:57.461512Z","shell.execute_reply":"2022-06-29T12:47:57.702048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = {'alpha' : [5e-4]}\nscoring = ['neg_mean_squared_error', 'r2']\nlassoCV = GridSearchCV(Lasso(), param_grid=parameters, scoring=scoring, refit='r2', cv=5).fit(X, y)\nprint(f\"Best Lasso Regressor is {lassoCV.best_estimator_},  and corresponding r2 score on prediction is {lassoCV.best_score_}\")\n\nysub_pred = np.expm1(lassoCV.predict(df_total.loc[n_train+1:]))\nsubmission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\nsubmission.to_csv('lassoCV_final.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:57.706942Z","iopub.execute_input":"2022-06-29T12:47:57.707527Z","iopub.status.idle":"2022-06-29T12:47:58.19643Z","shell.execute_reply.started":"2022-06-29T12:47:57.707486Z","shell.execute_reply":"2022-06-29T12:47:58.195683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:20px;\"> \n    Since we know the approximate range of parameters for ridge and lasso that yields best r2 score on the test data. Let us use it to find best combination of parameters that can be used in ElasticNet\n</span>","metadata":{}},{"cell_type":"code","source":"parameters = {'alpha' : [0.05], 'l1_ratio' : [1e-5]}\nscoring = ['neg_mean_squared_error', 'r2']\nenetCV = GridSearchCV(ElasticNet(), param_grid=parameters, scoring=scoring, refit='r2', cv=5).fit(X, y)\nprint(f\"Best ElasticNet Regressor is {enetCV.best_estimator_}, and corresponding r2 score on prediction is {enetCV.best_score_}\")\n\nysub_pred = np.expm1(enetCV.predict(df_total.loc[n_train+1:]))\nsubmission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\nsubmission.to_csv('enetCV_final.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:58.201209Z","iopub.execute_input":"2022-06-29T12:47:58.204616Z","iopub.status.idle":"2022-06-29T12:47:58.946283Z","shell.execute_reply.started":"2022-06-29T12:47:58.204567Z","shell.execute_reply":"2022-06-29T12:47:58.945557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking the best regressors","metadata":{}},{"cell_type":"code","source":"estimators = [('ridgeCV', ridgeCV), ('lassoCV', lassoCV)]\nstackCV = StackingRegressor(estimators=estimators, final_estimator=lassoCV)\nstackCV.fit(X_train, y_train)\ny_pred=stackCV.predict(X_test)\n\nysub_pred = np.expm1(stackCV.predict(df_total.loc[n_train+1:]))\nsubmission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\nsubmission.to_csv('stackCV_final.csv')\nr2 = r2_score(y_test, y_pred)\nrmse = np.sqrt(np.mean((y_test - y_pred)**2))\nr2, rmse","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:47:58.950809Z","iopub.execute_input":"2022-06-29T12:47:58.952952Z","iopub.status.idle":"2022-06-29T12:48:02.663348Z","shell.execute_reply.started":"2022-06-29T12:47:58.952909Z","shell.execute_reply":"2022-06-29T12:48:02.662615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residual = y_test - y_pred\nplt.scatter(y_pred, residual)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:48:02.668285Z","iopub.execute_input":"2022-06-29T12:48:02.668911Z","iopub.status.idle":"2022-06-29T12:48:02.851219Z","shell.execute_reply.started":"2022-06-29T12:48:02.668868Z","shell.execute_reply":"2022-06-29T12:48:02.850542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Boosting","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:48:02.85247Z","iopub.execute_input":"2022-06-29T12:48:02.852894Z","iopub.status.idle":"2022-06-29T12:48:05.077622Z","shell.execute_reply.started":"2022-06-29T12:48:02.852857Z","shell.execute_reply":"2022-06-29T12:48:05.076899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGB = XGBRegressor(n_estimators=1000,\n#                    learning_rate=0.05,\n#                    max_depth=10,\n#                    booster='dart',\n#                    objective='reg:squarederror').fit(X, y)\n\n\n# ysub_pred = np.exp(XGB.predict(df_total.loc[n_train+1:]))-1\n# submission = pd.DataFrame(data=ysub_pred, index=df_test.index, columns=['SalePrice'])\n# submission.to_csv('XGB_final.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:48:05.078909Z","iopub.execute_input":"2022-06-29T12:48:05.080778Z","iopub.status.idle":"2022-06-29T12:48:05.084877Z","shell.execute_reply.started":"2022-06-29T12:48:05.080741Z","shell.execute_reply":"2022-06-29T12:48:05.083621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = {'n_estimators' : [100,500,1000],\n              'learning_rate':[0.05,0.1,0.2,0.3],\n              'max_depth':[5,10,20]}\nscoring = ['neg_mean_squared_error', 'r2']\nXGBCV = GridSearchCV(XGBRegressor(), \n                       param_grid=parameters, scoring=scoring, \n                       refit='r2', cv=5).fit(X, y)\nprint(f\"Best XGB model is {XGBCV.best_estimator_},  and corresponding r2 score on prediction is {XGBCV.best_score_}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:29:53.976315Z","iopub.execute_input":"2022-06-29T12:29:53.977116Z","iopub.status.idle":"2022-06-29T12:39:36.495941Z","shell.execute_reply.started":"2022-06-29T12:29:53.977076Z","shell.execute_reply":"2022-06-29T12:39:36.494869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ysub_pred = np.expm1(XGBCV.predict(df_total.loc[n_train+1:]))\nsubmission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\nsubmission.to_csv('XGBCV_final.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LGBM =LGBMRegressor(n_estimators=1000,\n#                    learning_rate=0.05,\n#                    max_depth=6,\n#                    objective='regression').fit(X, y)\n\n\n# ysub_pred = np.exp(LGBM.predict(df_total.loc[n_train+1:]))-1\n# submission = pd.DataFrame(data=ysub_pred, index=df_test.index, columns=['SalePrice'])\n# submission.to_csv('LGBM_final.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T04:12:02.662861Z","iopub.execute_input":"2022-06-29T04:12:02.663194Z","iopub.status.idle":"2022-06-29T04:12:03.797246Z","shell.execute_reply.started":"2022-06-29T04:12:02.663162Z","shell.execute_reply":"2022-06-29T04:12:03.796458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = {'n_estimators' : [100,500,1000],\n              'learning_rate':[0.05,0.1,0.2,0.3],\n              'max_depth':[5,10,20]}\nscoring = ['neg_mean_squared_error', 'r2']\nLGBMCV = GridSearchCV(LGBMRegressor(), \n                       param_grid=parameters, scoring=scoring, \n                       refit='r2', cv=5).fit(X, y)\nprint(f\"Best XGB model is {XGBCV.best_estimator_},  and corresponding r2 score on prediction is {XGBCV.best_score_}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ysub_pred = np.expm1(LGBMCV.predict(df_total.loc[n_train+1:]))\nsubmission = pd.DataFrame(data=ysub_pred,index=df_test.index,columns=['SalePrice'])\nsubmission.to_csv('LGBMCV_final.csv')","metadata":{},"execution_count":null,"outputs":[]}]}